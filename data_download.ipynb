{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f253cb65-cd19-4239-995f-818cb855bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import hashlib\n",
    "import requests\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import librosa\n",
    "\n",
    "from utils.config import load_config "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f2c39-f201-4ac8-9238-201dd7e23762",
   "metadata": {},
   "source": [
    "## Get Data Paths from Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6887d260-635f-4392-b793-2001d16d2e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_paths': {'raw_data_path': '/app/data/raw',\n",
       "  'processed_data_path': '/app/data/processed',\n",
       "  'train_raw_path': '/app/data/raw/codecfake/train',\n",
       "  'label_raw_path': '/app/data/raw/codecfake/label'}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031b6cc-ad28-451a-8101-ce3fa5ab5a6a",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5098a8d2-c422-4264-b8f7-a4fc62e25f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_md5(filename, block_size=4096):\n",
    "    \"\"\"Calculate the MD5 checksum of a file.\"\"\"\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filename, 'rb') as f:\n",
    "        for block in iter(lambda: f.read(block_size), b\"\"):\n",
    "            md5.update(block)\n",
    "    return md5.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa5183c-4b0b-44db-aaae-7af4504cbfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_zenodo(direct_url, destination, expected_checksum):\n",
    "    \"\"\"Download a large file from Zenodo in chunks to avoid loading it all into memory.\"\"\"\n",
    "    if os.path.exists(destination):\n",
    "        if calculate_md5(destination) == expected_checksum:\n",
    "            print(f\"File already exists and is verified: {destination}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Checksum mismatch or file corrupted. Re-downloading: {destination}\")\n",
    "            os.remove(destination) \n",
    "    try:\n",
    "        with requests.get(direct_url, stream=True) as response:\n",
    "            response.raise_for_status() \n",
    "            with open(destination, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "        print(f\"Downloaded file to {destination}\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error: {e.response.status_code} {e.response.reason}\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Connection Error. Please check your internet connection.\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"The request timed out.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6433f39-856a-4ce1-b441-9f9d9f1f5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_parts(config):\n",
    "    repo_urls = [\n",
    "        \"https://zenodo.org/record/11171708\",\n",
    "        \"https://zenodo.org/records/11171720\",\n",
    "        \"https://zenodo.org/records/11171724\"\n",
    "    ]\n",
    "\n",
    "    files = [\n",
    "        [f'train_split.z{str(num).zfill(2)}' for num in range(1, 7)] + ['train_split.zip', 'label.zip'],\n",
    "        [f'train_split.z{str(num).zfill(2)}' for num in range(7, 15)],\n",
    "        [f'train_split.z{str(num).zfill(2)}' for num in range(15, 20)]\n",
    "    ]\n",
    "\n",
    "    checksums = {\n",
    "        'train_split.z01': '61423ee0c7e9991b7272b9e50b234439',\n",
    "        'train_split.z02': '938387b24c700fd3167caff5b6c4c2cc',\n",
    "        'train_split.z03': '6ed3919559200bfa2e09416816a748ab',\n",
    "        'train_split.z04': 'cffba4cd8a551e1da36e821e3db1137b',\n",
    "        'train_split.z05': 'c90ea493d8bfda6cf0fb7713e2bdf628',\n",
    "        'train_split.z06': 'a8363316c2db890f62d9a3f05ffa882b',\n",
    "        'train_split.z07': '8c89c7b19c2860dc360e53cf484f7844',\n",
    "        'train_split.z08': '069fb8d4ff70deafe2b23e70f67c255f',\n",
    "        'train_split.z09': '208fa914647e7519bf93eb04427e94ab',\n",
    "        'train_split.z10': '3441024afe061775a29d49292d6b94f6',\n",
    "        'train_split.z11': 'ef9b40ff9145bbe925944aa5a97a6060',\n",
    "        'train_split.z12': 'c9a30c2d9c4d0fd59c23058990e79c68',\n",
    "        'train_split.z13': '2fa3c4f13cad47c1a2c8da6b02593197',\n",
    "        'train_split.z14': 'd4b19b65945532a1192cfdaea45fe6e5',\n",
    "        'train_split.z15': 'f1416171017fe86806c1642f36865d22',\n",
    "        'train_split.z16': '4005490382925a7dde0df498831d4595',\n",
    "        'train_split.z17': '4aabe67a30484ab45919e58250f1d2c7',\n",
    "        'train_split.z18': '24fc5547fb782d59a8f94e53eb9fd2bc',\n",
    "        'train_split.z19': '2ded1a7fda786a04743923790a27f39f',\n",
    "        'train_split.zip': '600a9ab2c5d820004fecc0e67ac2f645',\n",
    "        'label.zip': '1886fa25a8e018307e709da28bdc57b2'\n",
    "    }\n",
    "    \n",
    "    # Set up the ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = []\n",
    "        for repo_url, file_group in zip(repo_urls, files):\n",
    "            for file in file_group:\n",
    "                download_url = f\"{repo_url}/files/{file}\"\n",
    "                zip_file_path = f\"{config['data_paths']['raw_data_path']}/codecfake/{file}\"\n",
    "                expected_checksum = checksums.get(file, None)\n",
    "                if expected_checksum:\n",
    "                    print(f\"Downloading {download_url} to {zip_file_path}\")\n",
    "                    futures.append(executor.submit(download_from_zenodo, download_url, zip_file_path, expected_checksum))\n",
    "    \n",
    "    # Wait for all futures to complete\n",
    "    for future in futures:\n",
    "        future.result() \n",
    "    print(\"All downloads completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e47e30-4cd0-4617-93fe-7bd64f80c5d0",
   "metadata": {},
   "source": [
    "## Download the 19-Part Archived Train Dataset\n",
    "\n",
    "Skip downloading if files are already present and verified, as verification is time-consuming due to each file being approximately 5GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a51510-acd0-4973-a89b-18a8c7a6f667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://zenodo.org/record/11171708/files/train_split.z01 to /app/data/raw/codecfake/train_split.z01\n",
      "Downloading https://zenodo.org/record/11171708/files/train_split.z02 to /app/data/raw/codecfake/train_split.z02\n",
      "Downloading https://zenodo.org/record/11171708/files/train_split.z03 to /app/data/raw/codecfake/train_split.z03\n",
      "Downloading https://zenodo.org/record/11171708/files/train_split.z04 to /app/data/raw/codecfake/train_split.z04\n",
      "Downloading https://zenodo.org/record/11171708/files/train_split.z05 to /app/data/raw/codecfake/train_split.z05\n",
      "Downloading https://zenodo.org/record/11171708/files/train_split.z06 to /app/data/raw/codecfake/train_split.z06\n",
      "Downloading https://zenodo.org/record/11171708/files/train_split.zip to /app/data/raw/codecfake/train_split.zip\n",
      "Downloading https://zenodo.org/record/11171708/files/label.zip to /app/data/raw/codecfake/label.zip\n",
      "Downloading https://zenodo.org/records/11171720/files/train_split.z07 to /app/data/raw/codecfake/train_split.z07\n",
      "Downloading https://zenodo.org/records/11171720/files/train_split.z08 to /app/data/raw/codecfake/train_split.z08\n",
      "Downloading https://zenodo.org/records/11171720/files/train_split.z09 to /app/data/raw/codecfake/train_split.z09\n",
      "Downloading https://zenodo.org/records/11171720/files/train_split.z10 to /app/data/raw/codecfake/train_split.z10\n",
      "Downloading https://zenodo.org/records/11171720/files/train_split.z11 to /app/data/raw/codecfake/train_split.z11\n",
      "Downloading https://zenodo.org/records/11171720/files/train_split.z12 to /app/data/raw/codecfake/train_split.z12\n",
      "Downloading https://zenodo.org/records/11171720/files/train_split.z13 to /app/data/raw/codecfake/train_split.z13\n",
      "Downloading https://zenodo.org/records/11171720/files/train_split.z14 to /app/data/raw/codecfake/train_split.z14\n",
      "Downloading https://zenodo.org/records/11171724/files/train_split.z15 to /app/data/raw/codecfake/train_split.z15\n",
      "Downloading https://zenodo.org/records/11171724/files/train_split.z16 to /app/data/raw/codecfake/train_split.z16\n",
      "Downloading https://zenodo.org/records/11171724/files/train_split.z17 to /app/data/raw/codecfake/train_split.z17\n",
      "Downloading https://zenodo.org/records/11171724/files/train_split.z18 to /app/data/raw/codecfake/train_split.z18\n",
      "Downloading https://zenodo.org/records/11171724/files/train_split.z19 to /app/data/raw/codecfake/train_split.z19\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z01\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z05\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z04\n",
      "File already exists and is verified: /app/data/raw/codecfake/label.zip\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z02\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z03\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.zip\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z06\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z08\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z09\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z07\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z10\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z11\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z13\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z12\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z14\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z15\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z16\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z17\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z18\n",
      "File already exists and is verified: /app/data/raw/codecfake/train_split.z19\n",
      "All downloads completed.\n"
     ]
    }
   ],
   "source": [
    "download_parts(config) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d428f74-948d-4498-9c70-654be556895b",
   "metadata": {},
   "source": [
    "## Extract Label files from label.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad1d7309-924e-44be-a1de-1a1430fad359",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_zip_path =  f\"{config['data_paths']['raw_data_path']}/codecfake/label.zip\"\n",
    "label_output_path = f\"{config['data_paths']['raw_data_path']}/codecfake/label\"\n",
    "if not os.path.exists(label_output_path):\n",
    "    unzip_file(label_zip_path, label_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f83e730-e39b-44a7-845d-d62ef0b6fb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSB13650058.wav real 0\n",
      "F01_SSB13650058.wav fake 1\n",
      "F02_SSB13650058.wav fake 2\n",
      "F03_SSB13650058.wav fake 3\n",
      "F04_SSB13650058.wav fake 4\n",
      "F05_SSB13650058.wav fake 5\n",
      "F06_SSB13650058.wav fake 6\n",
      "SSB13280206.wav real 0\n",
      "F01_SSB13280206.wav fake 1\n",
      "F02_SSB13280206.wav fake 2\n",
      "F03_SSB13280206.wav fake 3\n",
      "F04_SSB13280206.wav fake 4\n",
      "F05_SSB13280206.wav fake 5\n",
      "F06_SSB13280206.wav fake 6\n",
      "SSB07860395.wav real 0\n",
      "F01_SSB07860395.wav fake 1\n",
      "F02_SSB07860395.wav fake 2\n",
      "F03_SSB07860395.wav fake 3\n",
      "F04_SSB07860395.wav fake 4\n",
      "F05_SSB07860395.wav fake 5\n",
      "F06_SSB07860395.wav fake 6\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"{label_output_path}/train.txt\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for _ in range(21):\n",
    "        line = file.readline()\n",
    "        if not line: \n",
    "            break\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29216296-0289-4fa1-9c5d-805906ee9183",
   "metadata": {},
   "source": [
    "**Count Fakes Associated with Each Real Audio File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c77e6b-5316-4f2f-b46f-aa68391f8293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All real files have exactly 6 corresponding fake files.\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"{label_output_path}/train.txt\"\n",
    "\n",
    "real_to_fake_count = {}\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        filename = parts[0]\n",
    "        label = parts[1]\n",
    "        if label == \"real\":\n",
    "            real_to_fake_count[filename] = 0\n",
    "        elif label == \"fake\" and parts[2] != '0':\n",
    "            # Find the associated real file name by removing the first part (F01_, F02_, etc.)\n",
    "            real_file = filename[4:]\n",
    "            if real_file in real_to_fake_count:\n",
    "                real_to_fake_count[real_file] += 1\n",
    "            else:\n",
    "                print(f\"Warning: No real file found for {filename}\")\n",
    "\n",
    "consistent = True\n",
    "for real, count in real_to_fake_count.items():\n",
    "    if count != 6:\n",
    "        print(f\"Inconsistent count for {real}: {count} fakes found (expected 6).\")\n",
    "        consistent = False\n",
    "\n",
    "if consistent:\n",
    "    print(\"All real files have exactly 6 corresponding fake files.\")\n",
    "else:\n",
    "    print(\"Some real files do not have the correct number of corresponding fake files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81087caf-10d0-4c21-9f96-6afa849e81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105821 real audio files are there! Each has 6 corresponding fake files. So, totally 634926 audio files\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(real_to_fake_count)} real audio files are there! Each has 6 corresponding fake files. So, totally {len(real_to_fake_count)*6} audio files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be665eac-a4d3-49c6-a60d-52d6f6c43653",
   "metadata": {},
   "source": [
    "## Extract Specific Audio Files\n",
    "\n",
    "Extract only specific audio files, as the total size exceeds 101 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b8eabfb-b974-40d8-9d04-c0339f25f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_specific_files_with_7z(archive_path, output_path, files_to_extract, verbose=False):\n",
    "    \"\"\"Extract specific files from a multi-part zip archive using 7z from a subprocess.\"\"\"\n",
    "    try:\n",
    "        # Prepare the command to include specific files\n",
    "        command = ['7z', 'x', archive_path, f'-o{output_path}', '-aos'] + files_to_extract\n",
    "        if verbose:\n",
    "            subprocess.run(command, check=True)\n",
    "        else:\n",
    "            subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        if verbose:\n",
    "            print(f\"Extracted specified files to {output_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to extract files: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95febbed-49f8-44cd-a890-bb2d9777c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=C.UTF-8,Utf16=on,HugeFiles=on,64 bits,10 CPUs LE)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "1 file, 1395022381 bytes (1331 MiB)\n",
      "\n",
      "Extracting archive: /app/data/raw/codecfake/train_split.zip\n",
      "--\n",
      "Path = /app/data/raw/codecfake/train_split.zip\n",
      "Type = zip\n",
      "Physical Size = 1395022381\n",
      "Embedded Stub Size = 4\n",
      "64-bit = +\n",
      "Total Physical Size = 101009742381\n",
      "Multivolume = +\n",
      "Volume Index = 19\n",
      "Volumes = 20\n",
      "\n",
      "Everything is Ok\n",
      "\n",
      "Files: 14\n",
      "Size:       2948044\n",
      "Compressed: 101009742381\n",
      "Extracted specified files to /app/data/raw/codecfake/\n"
     ]
    }
   ],
   "source": [
    "archive_path = f\"{config['data_paths']['raw_data_path']}/codecfake/train_split.zip\"\n",
    "output_path = f\"{config['data_paths']['raw_data_path']}/codecfake/\"\n",
    "files_to_extract = [\n",
    "    # SSB13650058.wav\n",
    "    'train/SSB13650058.wav',\n",
    "    'train/F01_SSB13650058.wav',\n",
    "    'train/F02_SSB13650058.wav',\n",
    "    'train/F03_SSB13650058.wav',\n",
    "    'train/F04_SSB13650058.wav',\n",
    "    'train/F05_SSB13650058.wav',\n",
    "    'train/F06_SSB13650058.wav',\n",
    "    # SSB13280206.wav\n",
    "    'train/SSB13280206.wav',\n",
    "    'train/F01_SSB13280206.wav',\n",
    "    'train/F02_SSB13280206.wav',\n",
    "    'train/F03_SSB13280206.wav',\n",
    "    'train/F04_SSB13280206.wav',\n",
    "    'train/F05_SSB13280206.wav',\n",
    "    'train/F06_SSB13280206.wav',\n",
    "]\n",
    "\n",
    "extract_specific_files_with_7z(archive_path, output_path, files_to_extract, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c1482-dc94-4488-aedf-3acd3eae35c6",
   "metadata": {},
   "source": [
    "## Look at the train folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f2ec176-0d16-461f-8cad-7d0bcacf16e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F03_SSB13280206.wav',\n",
       " 'F05_SSB13650058.wav',\n",
       " 'SSB13650058.wav',\n",
       " 'F02_SSB13650058.wav',\n",
       " 'F04_SSB13280206.wav',\n",
       " 'SSB13280206.wav',\n",
       " 'F05_SSB13280206.wav',\n",
       " 'F03_SSB13650058.wav',\n",
       " 'F04_SSB13650058.wav',\n",
       " 'F02_SSB13280206.wav',\n",
       " 'F01_SSB13650058.wav',\n",
       " 'F06_SSB13650058.wav',\n",
       " 'F01_SSB13280206.wav',\n",
       " 'F06_SSB13280206.wav']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir_path = f\"{config['data_paths']['raw_data_path']}/codecfake/train\"\n",
    "os.listdir(train_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de271d5-5a68-43a8-a6fe-6958b03f3bcb",
   "metadata": {},
   "source": [
    "## Making sure sampling rate is same across all audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dafad253-8376-485b-9aa5-2ce415aec2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7/700 files. (1.00%)\n",
      "Processed 14/700 files. (2.00%)\n",
      "Processed 21/700 files. (3.00%)\n",
      "Processed 28/700 files. (4.00%)\n",
      "Processed 35/700 files. (5.00%)\n",
      "Processed 42/700 files. (6.00%)\n",
      "Processed 49/700 files. (7.00%)\n",
      "Processed 56/700 files. (8.00%)\n",
      "Processed 63/700 files. (9.00%)\n",
      "Processed 70/700 files. (10.00%)\n",
      "Processed 77/700 files. (11.00%)\n",
      "Processed 84/700 files. (12.00%)\n",
      "Processed 91/700 files. (13.00%)\n",
      "Processed 98/700 files. (14.00%)\n",
      "Processed 105/700 files. (15.00%)\n",
      "Processed 112/700 files. (16.00%)\n",
      "Processed 119/700 files. (17.00%)\n",
      "Processed 126/700 files. (18.00%)\n",
      "Processed 133/700 files. (19.00%)\n",
      "Processed 140/700 files. (20.00%)\n",
      "Processed 147/700 files. (21.00%)\n",
      "Processed 154/700 files. (22.00%)\n",
      "Processed 161/700 files. (23.00%)\n",
      "Processed 168/700 files. (24.00%)\n",
      "Processed 175/700 files. (25.00%)\n",
      "Processed 182/700 files. (26.00%)\n",
      "Processed 189/700 files. (27.00%)\n",
      "Processed 196/700 files. (28.00%)\n",
      "Processed 203/700 files. (29.00%)\n",
      "Processed 210/700 files. (30.00%)\n",
      "Processed 217/700 files. (31.00%)\n",
      "Processed 224/700 files. (32.00%)\n",
      "Processed 231/700 files. (33.00%)\n",
      "Processed 238/700 files. (34.00%)\n",
      "Processed 245/700 files. (35.00%)\n",
      "Processed 252/700 files. (36.00%)\n",
      "Processed 259/700 files. (37.00%)\n",
      "Processed 266/700 files. (38.00%)\n",
      "Processed 273/700 files. (39.00%)\n",
      "Processed 280/700 files. (40.00%)\n",
      "Processed 287/700 files. (41.00%)\n",
      "Processed 294/700 files. (42.00%)\n",
      "Processed 301/700 files. (43.00%)\n",
      "Processed 308/700 files. (44.00%)\n",
      "Processed 315/700 files. (45.00%)\n",
      "Processed 322/700 files. (46.00%)\n",
      "Processed 329/700 files. (47.00%)\n",
      "Processed 336/700 files. (48.00%)\n",
      "Processed 343/700 files. (49.00%)\n",
      "Processed 350/700 files. (50.00%)\n",
      "Processed 357/700 files. (51.00%)\n",
      "Processed 364/700 files. (52.00%)\n",
      "Processed 371/700 files. (53.00%)\n",
      "Processed 378/700 files. (54.00%)\n",
      "Processed 385/700 files. (55.00%)\n",
      "Processed 392/700 files. (56.00%)\n",
      "Processed 399/700 files. (57.00%)\n",
      "Processed 406/700 files. (58.00%)\n",
      "Processed 413/700 files. (59.00%)\n",
      "Processed 420/700 files. (60.00%)\n",
      "Processed 427/700 files. (61.00%)\n",
      "Processed 434/700 files. (62.00%)\n",
      "Processed 441/700 files. (63.00%)\n",
      "Processed 448/700 files. (64.00%)\n",
      "Processed 455/700 files. (65.00%)\n",
      "Processed 462/700 files. (66.00%)\n",
      "Processed 469/700 files. (67.00%)\n",
      "Processed 476/700 files. (68.00%)\n",
      "Processed 483/700 files. (69.00%)\n",
      "Processed 490/700 files. (70.00%)\n",
      "Processed 497/700 files. (71.00%)\n",
      "Processed 504/700 files. (72.00%)\n",
      "Processed 511/700 files. (73.00%)\n",
      "Processed 518/700 files. (74.00%)\n",
      "Processed 525/700 files. (75.00%)\n",
      "Processed 532/700 files. (76.00%)\n",
      "Processed 539/700 files. (77.00%)\n",
      "Processed 546/700 files. (78.00%)\n",
      "Processed 553/700 files. (79.00%)\n",
      "Processed 560/700 files. (80.00%)\n",
      "Processed 567/700 files. (81.00%)\n",
      "Processed 574/700 files. (82.00%)\n",
      "Processed 581/700 files. (83.00%)\n",
      "Processed 588/700 files. (84.00%)\n",
      "Processed 595/700 files. (85.00%)\n",
      "Processed 602/700 files. (86.00%)\n",
      "Processed 609/700 files. (87.00%)\n",
      "Processed 616/700 files. (88.00%)\n",
      "Processed 623/700 files. (89.00%)\n",
      "Processed 630/700 files. (90.00%)\n",
      "Processed 637/700 files. (91.00%)\n",
      "Processed 644/700 files. (92.00%)\n",
      "Processed 651/700 files. (93.00%)\n",
      "Processed 658/700 files. (94.00%)\n",
      "Processed 665/700 files. (95.00%)\n",
      "Processed 672/700 files. (96.00%)\n",
      "Processed 679/700 files. (97.00%)\n",
      "Processed 686/700 files. (98.00%)\n",
      "Processed 693/700 files. (99.00%)\n",
      "Processed 700/700 files. (100.00%)\n"
     ]
    }
   ],
   "source": [
    "file_to_sr = {}\n",
    "archive_path = f\"{config['data_paths']['raw_data_path']}/codecfake/train_split.zip\"\n",
    "output_path = f\"{config['data_paths']['raw_data_path']}/codecfake/\"\n",
    "\n",
    "def process_file(file_path, output_path):\n",
    "    \"\"\"Function to process a single file to extract it and read its sampling rate.\"\"\"\n",
    "    extract_specific_files_with_7z(archive_path, output_path, [file_path])\n",
    "    full_path = os.path.join(output_path, file_path)\n",
    "    y, sr = librosa.load(full_path, sr=None)\n",
    "    ################ DELETE THE FILE TO SAVE SPACE ###################\n",
    "    os.remove(full_path)\n",
    "    ##################################################################\n",
    "    return file_path, sr\n",
    "\n",
    "\n",
    "def process_files_concurrently(all_files_to_process):\n",
    "    num_files = len(all_files_to_process)\n",
    "    update_interval = num_files // 100\n",
    "    count = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_file = {executor.submit(process_file, file, output_path): file for file in all_files_to_process}\n",
    "        for future in as_completed(future_to_file):\n",
    "            file, sr = future.result()\n",
    "            file_to_sr[file] = sr\n",
    "            count += 1\n",
    "            if count % update_interval == 0:\n",
    "                print(f\"Processed {count}/{num_files} files. ({(count/num_files)*100:.2f}%)\")\n",
    "\n",
    "all_files_to_process = []\n",
    "for real in random.sample(list(real_to_fake_count.keys()), 100): # real_to_fake_count.keys()\n",
    "    real_file_path = os.path.join('train', real)\n",
    "    fake_file_paths = [os.path.join('train', f'F{str(num).zfill(2)}_{real}') for num in range(1, 7)]\n",
    "    all_files_to_process.extend([real_file_path] + fake_file_paths)\n",
    "    \n",
    "process_files_concurrently(all_files_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d91c3e1-e42c-49c0-a3e6-5d7b1331da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_map = {\n",
    "    'p': {\n",
    "        'real': 48000,\n",
    "        'F01': 16000,\n",
    "        'F02': 16000,\n",
    "        'F03': 16000,\n",
    "        'F04': 24000,\n",
    "        'F05': 48000,\n",
    "        'F06': 16000,\n",
    "    },\n",
    "    'SSB': {\n",
    "        'real': 44100,\n",
    "        'F01': 16000,\n",
    "        'F02': 16000,\n",
    "        'F03': 16000,\n",
    "        'F04': 24000,\n",
    "        'F05': 48000,\n",
    "        'F06': 24000,\n",
    "    }\n",
    "}\n",
    "for file in file_to_sr.keys():\n",
    "    # Real Audios\n",
    "    if not file.replace('train/', '').startswith('F0'):\n",
    "        if file.replace('train/', '').startswith('p') and file_to_sr[file] != sr_map['p']['real']:\n",
    "            print('Error', file, file_to_sr[file])\n",
    "        elif file.replace('train/', '').startswith('SSB') and file_to_sr[file] != sr_map['SSB']['real']:\n",
    "            print('Error', file, file_to_sr[file])\n",
    "\n",
    "    # Fake Audios\n",
    "    else:\n",
    "        fake_number = file.replace('train/', '')[:3]\n",
    "        if file.replace(f'train/{fake_number}_', '').startswith('p') and file_to_sr[file] != sr_map['p'][fake_number]:\n",
    "            print('Error', file, file_to_sr[file])\n",
    "        elif file.replace(f'train/{fake_number}_', '').startswith('SSB') and file_to_sr[file] != sr_map['SSB'][fake_number]:\n",
    "            print('Error', file, file_to_sr[file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5c03d-a465-46c0-912a-6b641106cb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa96f2-0720-43e5-9c41-665f1b441ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2f919-b85f-45e2-a3bf-2ecf0028acd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11508634-00f2-463d-8836-c87fd549555f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22866136-968b-4278-ad7d-369f25abf539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdcf45f-55b2-4d90-a393-b4e7e64a6ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d84c8-da07-4a5b-88d7-8cc51e304fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
